kurt = kurtosis(df$len)
md = median(df$len)
iqr = IQR(df$len)
return(c(kurt, md, iqr))
}
smry = sapply(tg, FUN = eda_kmi)
row.names(smry) = c("sample kurtosis", "sample median", "sample IQR")
smry
compute_kmi<-function(vec){
kurt = kurtosis(vec)
md = median(vec)
iqr = IQR(vec)
return(c(kurt,md,iqr))
}
bvjack <- function(vec){
n = length(vec)-1
jvec = vec[1:n]
est = vec[n+1]
bjack = (n-1)*(mean(jvec)-est)
vjack = (n-1)^2/n*var(jvec)
return(c(bjack, vjack))
}
jackknife <- function(df){
len = df$len
n = length(len)
kmi = matrix(NA, nrow = n+1, ncol = 3)
colnames(kmi) = c("kurtosis","median","IQR")
kmi[n+1,] = compute_kmi(len)
for (j in 1:n){
kmi[j,] = compute_kmi(len[-j])
}
bv_mat = apply(kmi,2,bvjack)
return(bv_mat)
}
bv = lapply(tg, FUN = jackknife)
row_rename<-function(df){
rownames(df)=c("bias", "variance")
return(df)
}
bv = lapply(bv, FUN = row_rename)
bv
set.seed(1)
B = 1000
boots<-function(df){
n = nrow(df)
boot_mat = matrix(NA, nrow = B, ncol = 3)
colnames(boot_mat) = c("kurtosis", "median", "IQR")
for (b in 1:B){
rnd_ind = sample(1:n, size = n, replace =TRUE)
boot_vec = df$len[rnd_ind]
boot_mat[b,] = compute_kmi(boot_vec)
}
kmi0 = compute_kmi(df$len)
boot_bias = apply(boot_mat, 2, mean)-kmi0
boot_variance = apply(boot_mat, 2, var)
boot_bv = rbind(boot_bias, boot_variance)
rownames(boot_bv) = c("bias", "variance")
return(boot_bv)
}
bv_boots = lapply(tg, FUN = boots)
bv_boots
set.seed(1)
boots_iqr<-function(df){
n = nrow(df)
iqr_vec = rep(NA, B)
for (b in 1:B){
rnd_ind = sample(1:n, size = n, replace =TRUE)
boot_vec = df$len[rnd_ind]
iqr_vec[b] = IQR(boot_vec)
}
return(iqr_vec)
}
iqr_diff = sapply(tg, boots_iqr)
iqr_diff = iqr_diff[,1] - iqr_diff[,2]
ci = quantile(iqr_diff, probs = c(0.025, 0.975))
ci
samp_iqr = sapply(tg, boots_iqr)
samp_iqr
?pnorm
qnorm(0.975, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
lower = mean(iqr_diff) - 1.96*sd(iqr_diff)
upper = mean(iqr_diff) + 1.96*sd(iqr_diff)
c(lower, upper)
library(ggplot2)
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
setwd("~/GitHub/Fall2018-Proj1-JudyCom/output")
data = read.csv("processed_moments.csv",as.is = TRUE)
df = data[,c(3,9,10,11)]
category = as.factor(df$predicted_category)
n = nrow(data)
ind_3m = which(df$reflection == "3m")
data_3m = df[ind_3m,]
ind_24h = which(df$reflection == "24h")
data_24h = df[ind_24h,]
n_3m = nrow(data_3m)
n_24h = nrow(data_24h)
# cat_freq = table(category)/n
# cat_freq
# barplot(cat_freq, main = "Category Frequency")
# Create corpus
corpus=Corpus(VectorSource(data$text))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
# Remove stopwords
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
library(twitteR)
install.packages("twitteR")
library(twitteR)
library(ROAuth)
install.packages("ROAuth")
install.packages("RCurl")
install.packages("stringr")
install.packages("stringr")
install.packages("stringr")
install.packages("stringr")
library(tm)
install.packages("ggmap")
library(wordcloud)
install.packages("wordcloud")
library(plyr)
library(tm)
library(twitteR)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
# Create corpus
corpus=Corpus(VectorSource(data$text))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
# Remove stopwords
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
?wordcloud
wordcloud(corpus, min.freq=100, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=5000, random.order=F,colors=col)
library(twitteR)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
key="hidden"
secret="hidden"
setwd("/text_mining_and_web_scraping")
setwd("~/GitHub/Fall2018-Proj1-JudyCom/output")
key="hidden"
secret="hidden"
setwd("~/GitHub/Fall2018-Proj1-JudyCom/output")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="/text_mining_and_web_scraping/cacert.pem",
method="auto")
key="hidden"
secret="hidden"
setwd("~/GitHub/Fall2018-Proj1-JudyCom/output")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="~/GitHub/Fall2018-Proj1-JudyCom/output/cacert.pem",
method="auto")
authenticate <- OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL="https://api.twitter.com/oauth/request_token",
accessURL="https://api.twitter.com/oauth/access_token",
authURL="https://api.twitter.com/oauth/authorize")
setup_twitter_oauth(key, secret)
# Load libraries:
library(dplyr)
library(data.table)
install.packages("data.table")
# Load libraries:
library(dplyr)
library(data.table)
library(DT)
library(ggplot2)
library(plotly)
install.packages("plotly")
library(scatterD3)
install.packages("scatterD3")
library(grid)
library(gridExtra)
# Load libraries:
library(dplyr)
library(data.table)
library(DT)
library(ggplot2)
library(plotly)
library(scatterD3)
library(grid)
library(gridExtra)
# Load data and select variables:
selected.variables = c("NATIVITY","ST","SCHL", "YOEP", "AGEP","PWGTP","INDP","SOCP","COW","ESR","POBP",
"POVPIP","WAGP","FINCP","HINCP","FFINCP","FHINCP","OIP")
hus.a.data = fread("../data/ss14pusa.csv", select = selected.variables)
# Get country names and state abbreviations:
countries = fread("../data/countrynames.csv")
db = db %>%
left_join(., countries, by = c("POBP" = "code")) %>%
rename(COB_name = name)
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
?inner_join
wordcount
?wordcount
hm_data$text
class(hm_data$text)
?sapply
?mutate
head(airquality)
mutate(airquality, Ozone = -Ozone)
?count
?%in%
?fct_recode
x<- factor(c("apple", "bear", "banana", "dear"))
fct_recode(x, fruit = "apple", fruit = "banana
)
)
)
]
]
()
[]
?fct_recode
<-5
=0
x <- factor(c("apple", "bear", "banana", "dear"))
fct_recode(x, fruit = "apple", fruit = "banana")
x <- factor(c("apple", "bear", "banana, "dear"))
x <- factor(c("apple", "bear", "banana, "dear"))
x <- factor(c("apple", "bear", "banana", "dear"))
x
fct_recode(x, fruit = "apple", fruit = "banana")
head(hm_data)
names(hm_data)
count
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- read_csv("../output/processed_moments.csv")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
hm_data <- hm_data %>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
sapply(hm_data$text, wordcount)
class(hm_data)
hm_data
datatable(hm_data)
?datatable
hm_data
datatable(hm_data)
?unnest_tokens
word
?word
bag_of_words <-  hm_data %>%
unnest_tokens(word, text)
bag_of_words
bag_of_words$word
head(bag_of_words)
word_count <- bag_of_words %>%
count(word, sort = TRUE)
word_count
?unnest_tokens
hm_bigrams <- hm_data %>%
filter(count != 1) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
hm_bigrams
hm_bigrams$bigram
?separate
bigram_counts <- hm_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
bigram_counts
?count
head(bigram_counts)
tail(bigram_counts)
tail(bigram_counts)
?navbarPage
ui <- navbarPage("What makes people happy?",
tabPanel("Overview",
titlePanel(h1("Most Frequent Occurrences",
align = "center")),
sidebarLayout(
sidebarPanel(
sliderInput(inputId = "topWordcloud",
label = "Number of terms for word cloud:",
min = 5,
max = 100,
value = 50),
br(),
br(),
checkboxInput(inputId = "topFreqB",
label = "Plot Bar Chart",
value = F),
sliderInput(inputId = "topBarchart",
label = "Number of terms for bar chart:",
min = 1,
max = 25,
value = 10),
br(),
br(),
checkboxInput(inputId = "topFreqN",
label = "Plot Network Graph",
value = F),
sliderInput(inputId = "topNetwork",
label = "Number of edges for network graph:",
min = 1,
max = 150,
value = 50)
),
mainPanel(
wordcloud2Output(outputId = "WC"),
plotOutput(outputId = "figure")
)
)
),
tabPanel("Individual Terms",
titlePanel(h1("Comparison of Proportions",
align = "center")),
sidebarLayout(
sidebarPanel(
selectInput(inputId = "attribute",
label = "Select the attribute:",
choices = c("Gender" = "gender",
"Marital Status" = "marital",
"Parenthood" = "parenthood",
"Reflection Period" = "reflection_period")
)
),
mainPanel(
plotOutput(outputId = "scatter")
)
)
),
tabPanel("Pair of Words",
titlePanel(h1("Most Frequent Bigrams",
align = "center")),
sidebarLayout(
sidebarPanel(
selectInput(inputId = "factor",
label = "Select the attribute:",
choices = c("Gender" = "gender",
"Marital Status" = "marital",
"Parenthood" = "parenthood",
"Reflection Period" = "reflection_period")
),
numericInput(inputId = "topBigrams",
label = "Number of top pairs to view:",
min = 1,
max = 25,
value = 10)
),
mainPanel(
plotOutput(outputId = "bar")
)
)
),
tabPanel("Data",
DT::dataTableOutput("table")
)
)
server <- function(input, output, session) {
pt1 <- reactive({
if(!input$topFreqB) return(NULL)
word_count %>%
slice(1:input$topBarchart) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
ylab("Word Frequency")+
coord_flip()
})
pt2 <- reactive({
if(!input$topFreqN) return(NULL)
bigram_graph <- bigram_counts %>%
slice(1:input$topNetwork) %>%
graph_from_data_frame()
set.seed(123)
x <- grid::arrow(type = "closed", length = unit(.1, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = x, end_cap = circle(.05, 'inches')) +
geom_node_point(color = "skyblue", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
})
output$WC <- renderWordcloud2({
word_count %>%
slice(1:input$topWordcloud) %>%
wordcloud2(size = 0.6,
rotateRatio = 0)
})
output$figure <- renderPlot(height = 500, width = 500, {
ptlist <- list(pt1(),pt2())
ptlist <- ptlist[!sapply(ptlist, is.null)]
if(length(ptlist)==0) return(NULL)
lay <- rbind(c(1,1),
c(2,2))
grid.arrange(grobs = ptlist, layout_matrix = lay)
})
selectedAttribute <- reactive({
list(atr = input$attribute)
})
output$scatter <- renderPlot({
temp <- bag_of_words %>%
count(!!as.name(selectedAttribute()$atr), word) %>%
group_by(!!as.name(selectedAttribute()$atr)) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(!!as.name(selectedAttribute()$atr), proportion)
ggplot(temp,
aes_string(x = colnames(temp)[2], y = colnames(temp)[3]),
color = abs(colnames(temp)[3] - colnames(temp)[2])) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 1, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
theme(legend.position="none")
})
selectedBigram <- reactive({
list(var = input$factor)
})
output$bar <- renderPlot({
hm_bigrams %>%
count(!!as.name(selectedBigram()$var), bigram, sort = TRUE) %>%
group_by(!!as.name(selectedBigram()$var)) %>%
top_n(input$topBigrams) %>%
ungroup() %>%
mutate(bigram = reorder(bigram, n)) %>%
ggplot(aes(bigram, n, fill = !!as.name(selectedBigram()$var))) +
geom_col(show.legend = FALSE) +
facet_wrap(as.formula(paste("~", selectedBigram()$var)), ncol = 2, scales = "free") +
coord_flip()
})
output$table <- DT::renderDataTable({
DT::datatable(hm_data)
})
}
shinyApp(ui, server)
?titlePanel
?h1
?wordcloud2Output
?plotOutput
shinyApp(ui, server)
